
import pandas as pd
from google.oauth2 import service_account
import os
from google.cloud import bigquery
from bq_actions import save_dataframe_to_bq

pathdir = os.path.dirname(os.path.abspath(__file__))
private_key = os.path.join(pathdir, '..', '..', '..', 'key.json')

credentials = service_account.Credentials.from_service_account_file(
    private_key
)

print("👀 Loading job details file from BQ")
df_detail = pd.read_gbq('select * from dash_work.job_details', credentials=credentials)
print(f"✅ JOb details file downloaded from BQ! Data size = {df_detail.shape}")

print("🪚 🪠 🧽 Starting data cleaning process ")
#drop columns in dataframe
df_detail=df_detail.drop(["Unnamed_0", "Unnamed_1", "branche", "eintrittsdatum", "ersteVeroeffentlichungsdatum","titel"], axis=1)

size_before = df_detail.shape[0]
#drop duplicates
df_detail.drop_duplicates(subset=['refnr'], inplace=True)
dup_num = (size_before-df_detail.shape[0])
print (f"Dropped {dup_num} duplicates")

#add timestamp of access minus today in number of days
#df_detail['days_since_posting'] = (pd.to_datetime('today') -pd.to_datetime(df_detail['aktuelleVeroeffentlichungsdatum'])).dt.days

#keep only df["arbeitsort.land"] == "Deutschland"
df_detail = df_detail[df_detail["arbeitsorte_land"] == "Deutschland"]

# fill Arbeitsort_ort with "hybrid" if NaN
df_detail['arbeitsorte_ort'].fillna("regional/hybrid", inplace=True)

print("🧚‍♀️ Data is clean now!")

print("🏋️‍♀️ Final step: master mapping...")
df_mapping = pd.read_csv('MASTER_MAPPING_attempt.csv', sep=";", encoding='latin-1')
merged_df = df_detail.merge(df_mapping, how='inner', left_on='arbeitsorte_ort', right_on='arbeitsort.plz')
merged_df = merged_df.rename(columns={'arbeitsort.plz': 'arbeitsort_ort', 'landkreis_georef': 'landkreis'})

print("🔥 Mapping complete! Loading dataframe to BQ!")

save_dataframe_to_bq(merged_df, 'wagon-bootcamp-384015', 'dash_work', 'master_all_jobs1', truncate=True)
